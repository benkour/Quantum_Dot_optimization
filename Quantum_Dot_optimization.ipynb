{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import GPy\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from warnings import catch_warnings\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import sys\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "tfk = tf.keras\n",
    "from sklearn import model_selection\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import  Input, Dense\n",
    "from keras.models import Model\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ed0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(a):\n",
    "    # the +1 is to normalize in [1 2] as otherwise the relative error would encounter division by 0\n",
    "    return (a - a.min(axis = 0))/(a.max(axis = 0) - a.min(axis = 0)) \n",
    "\n",
    "def customLoss(y_actual,y_pred): \n",
    "    custom_loss = tf.math.divide( abs(y_pred-y_actual) , y_actual)\n",
    "    custom_loss=tf.reduce_mean(custom_loss)\n",
    "    return custom_loss\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions.flatten() - targets.flatten()) ** 2).mean())\n",
    "\n",
    "def model_evaluation(model,X_test,y_test):\n",
    "    predictions_film = model.predict(X_test)\n",
    "\n",
    "    idx_test_film = np.argmax(predictions_film, axis=-1)\n",
    "    idx_test_t_film = np.argmax(y_test, axis=-1)\n",
    "    error_film = idx_test_film - idx_test_t_film\n",
    "    print(100 - 100*np.count_nonzero(error_film)/error_film.shape[0])\n",
    "    plt.hist(idx_test_film - idx_test_t_film )\n",
    "    plt.show()\n",
    "\n",
    "    cnf_matrix = confusion_matrix(np.argmax(predictions_film, axis=-1),np.argmax(y_test, axis=-1))\n",
    "    print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c78802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = os.path.abspath(\"data\") \n",
    "parameters = pd.read_csv(k + '/' +'Compositions'+'.csv')\n",
    "pbr2 = np.array((parameters['PbBr2  (mL)'].astype(float)))\n",
    "oleate = np.array((parameters['Cs-oleate (mL)'].astype(float)))\n",
    "aceton = np.array((parameters['Aceton (mL)'].astype(float)))\n",
    "\n",
    "pbr2 = parameters['PbBr2  (mL)']\n",
    "oleate = parameters['Cs-oleate (mL)']\n",
    "aceton = parameters['Aceton (mL)']\n",
    "\n",
    "aceton = aceton\n",
    "inputs = np.array([pbr2,oleate,aceton]).T\n",
    "file_nr = len(glob.glob(k + '/'+\"*.csv\"))\n",
    "file_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cde2a",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbf91c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_median_metric = []\n",
    "hwhm = []\n",
    "max_peak = []\n",
    "for i in range(1,file_nr): \n",
    "    a = pd.read_csv(k + '/' + str(i)+'.csv')\n",
    "    a = a.iloc[1:]\n",
    "    d = np.array((a['S1c / R1c'].astype(float)))\n",
    "    w = np.array(a['Wavelength'].astype(float))\n",
    "    d = np.reshape(d, (d.shape[0],1))\n",
    "    w = np.reshape(w, (w.shape[0],1))\n",
    "    \n",
    "    \n",
    "    d_max = np.max(d)\n",
    "    d = d/d_max\n",
    "    max_index = np.argmax(d, axis=0)\n",
    "    \n",
    "    \n",
    "    init = 0\n",
    "    ending = d.shape[0]\n",
    "    for o in range(0,max_index[0]):\n",
    "        if d[o] >0.05:\n",
    "            init = o\n",
    "            break\n",
    "    for o in range(max_index[0], d.shape[0]):\n",
    "        if d[o]<0.05:\n",
    "            ending = o\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    max_index = np.argmax(d, axis=0)\n",
    "    \n",
    "    data_range = min((max_index), w.shape[0]-int(max_index))\n",
    "    \n",
    "    \n",
    "    d_left = d[int(init):int(max_index)]\n",
    "    w_left = w[int(init):int(max_index)]\n",
    "\n",
    "    d_right = d[int(max_index): int(ending)]\n",
    "    w_right = w[int(max_index): int(ending)]\n",
    "    d_right = d_right[::-1]\n",
    "    w_right = w_right[::-1]\n",
    "\n",
    "    mean = w[np.argmax(d)]\n",
    "    median_cumsum = np.cumsum(d)\n",
    "    index = np.argmax(median_cumsum>median_cumsum[-1]*0.5)\n",
    "    median = w[index]\n",
    "\n",
    "    mean_median_metric_sample = 100*abs((mean-median)/mean)\n",
    "    mean_median_metric.append(mean_median_metric_sample)\n",
    "\n",
    "    first_half = np.argwhere(np.array(d) >  0.5*np.max(d))[0][0] - 1\n",
    "    second_half = np.argwhere(d[np.argmax(d):] <  0.5*np.max(d))[0][0] + np.argmax(d)-1\n",
    "\n",
    "    hwhm_sample =abs( w[first_half] - w[second_half])\n",
    "    hwhm.append(hwhm_sample)\n",
    "    max_peak.append(w[np.argmax(d)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8722545",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwhm = np.array(hwhm)\n",
    "mean_median_metric = np.array(mean_median_metric)\n",
    "outputs = normalization(hwhm) + normalization(mean_median_metric)\n",
    "outputs  = 1/ outputs\n",
    "outputs = normalization(outputs)\n",
    "outputs_peak = np.array(max_peak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2182c40",
   "metadata": {},
   "source": [
    "# remove double peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27261fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 3)\n",
      "(174, 1)\n",
      "(174, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs_pruned = inputs.copy()\n",
    "outputs_pruned = outputs.copy()\n",
    "outputs_peak_pruned = outputs_peak.copy()\n",
    "\n",
    "\n",
    "indices_of_peaks = np.arange(0,file_nr)\n",
    "indices_of_peaks_pruned = indices_of_peaks.copy()\n",
    "\n",
    "double_peak_samples = [220,219,216,215,214,209,182,180,179,177,174,173,172,170,167, 166,165,163,161,160,159,158,157,156, 153,152,149,143,137,134,126,125,124,123,121,117,115,105,104,103,87,69,39,38,22,18]\n",
    "double_peak_samples = [x - 1 for x in double_peak_samples]\n",
    "# print(double_peak_samples)\n",
    "for i in double_peak_samples :\n",
    "    indices_of_peaks_pruned = np.delete(indices_of_peaks_pruned, i, axis=0)\n",
    "    inputs_pruned = np.delete(inputs_pruned, i, axis=0)\n",
    "    outputs_pruned = np.delete(outputs_pruned, i, axis=0)\n",
    "    outputs_peak_pruned = np.delete(outputs_peak_pruned, i, axis=0)\n",
    "\n",
    "print(inputs_pruned.shape)\n",
    "print(outputs_pruned.shape)\n",
    "print(outputs_peak_pruned.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b3a3",
   "metadata": {},
   "source": [
    "# Peak Position Prediction NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_peak (X_train): #X_train is our training input\n",
    "\n",
    "    inp_1 = Input(shape=(X_train.shape[1],))  #setting the size of the input layer\n",
    "    initial = 'he_uniform'\n",
    "    x = Dense(5,kernel_initializer= initial, activation = 'elu', bias_initializer=initial)(inp_1)\n",
    "    x = Dense(5,kernel_initializer= initial, activation = 'relu', bias_initializer=initial)(x)\n",
    "    x = Dense(5,kernel_initializer= initial, activation = 'relu', bias_initializer=initial)(x)\n",
    "    x = Dense(1,kernel_initializer= initial,  activation = 'elu',bias_initializer=initial)(x)\n",
    "\n",
    "    return Model(inputs=inp_1, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a2ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr_schedule = tfk.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.99)    \n",
    "\n",
    "predictions_test_batch = []\n",
    "y_true_batch = []\n",
    "for i in range(0,30):\n",
    "    X_train_peak, X_test_peak, y_train_peak, y_test_peak = model_selection.train_test_split(inputs_pruned,outputs_peak_pruned, test_size=0.02,random_state = i ) \n",
    "\n",
    "    opt = tfk.optimizers.RMSprop(learning_rate=lr_schedule,clipvalue=0.8)\n",
    "    #model.summary()\n",
    "    peak_model = Sequential()  # instantiate the model \n",
    "    peak_model = model_peak(X_train_peak)  #give it the parameters given in the function model_dam\n",
    "\n",
    "\n",
    "\n",
    "    peak_model.compile(loss = customLoss, optimizer= opt, metrics=[customLoss]) #compile the model \n",
    "\n",
    "\n",
    "    checkpoint_filepath = '/tmp/checkpoint'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "\n",
    "    history_callback = peak_model.fit(X_train_peak, y_train_peak,validation_data=(X_test_peak, y_test_peak),epochs=1000, verbose = 0)\n",
    "\n",
    "    predictions_test_batch.append(peak_model.predict(X_test_peak))\n",
    "    y_true_batch.append(y_test_peak)\n",
    "\n",
    "#     print(i)\n",
    "y_true_batch = np.array(y_true_batch)\n",
    "predictions_test_batch = np.array(predictions_test_batch)\n",
    "predictions_test_batch = np.reshape(predictions_test_batch, (predictions_test_batch.shape[0]*predictions_test_batch.shape[1],1))\n",
    "y_true_batch = np.reshape(y_true_batch, (y_true_batch.shape[0]*y_true_batch.shape[1],1))\n",
    "\n",
    "print(\"median error % \", np.median(100*abs((predictions_test_batch-y_true_batch)/y_true_batch)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856b2f2",
   "metadata": {},
   "source": [
    "# Classification of formability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636478b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape)\n",
    "double_peak_indicator = np.zeros((inputs.shape[0],1))\n",
    "for i in double_peak_samples:\n",
    "    double_peak_indicator[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state=seed_value, shuffle=True)\n",
    "skf.get_n_splits(inputs, double_peak_indicator)\n",
    "dp_pred = []\n",
    "dp_true = []\n",
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(inputs, double_peak_indicator):\n",
    "    double_peak_RF = RandomForestClassifier(max_depth = 150,min_samples_split = 10, random_state= 42)\n",
    "    input_for_double_train, input_for_double_test = inputs[train_index], inputs[test_index]\n",
    "    double_peak_train, double_peak_test = double_peak_indicator[train_index], double_peak_indicator[test_index]\n",
    "    \n",
    "    double_peak_RF.fit(input_for_double_train, double_peak_train)\n",
    "\n",
    "    prediction = double_peak_RF.predict(input_for_double_test)\n",
    "    prediction = np.array(prediction)\n",
    "    prediction = np.reshape(prediction, (prediction.shape[0],1))\n",
    "    try:\n",
    "        dp_pred = np.vstack((dp_pred,prediction ))\n",
    "    except:\n",
    "        dp_pred = prediction\n",
    "    try:\n",
    "        dp_true = np.vstack((dp_true,double_peak_test ))\n",
    "    except:\n",
    "        dp_true = double_peak_test\n",
    "    \n",
    "dp_pred = np.array(dp_pred)   \n",
    "dp_true = np.array(dp_true)   \n",
    "\n",
    "dp_pred = np.reshape(dp_pred, (dp_pred.shape[0],1))\n",
    "cnf_matrix = confusion_matrix(dp_true,dp_pred)\n",
    "print()\n",
    "print(\"random forest  \")\n",
    "print(\"error_rf = \", 100*len(np.nonzero(dp_pred-dp_true)[0])/dp_true.shape[0] )\n",
    "print(\"confusion matrix\")\n",
    "print(cnf_matrix)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2a6f6",
   "metadata": {},
   "source": [
    "# Objective Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ae5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "truth = []\n",
    "std = []\n",
    "\n",
    "\n",
    "sigma_f, l = 0.5,1\n",
    "for i in range(30):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(inputs_pruned,outputs_pruned, test_size=0.02,random_state = i )\n",
    "\n",
    "    kernel = GPy.kern.RBF(1, sigma_f, l)\n",
    "    kernel += GPy.kern.Matern32(input_dim=inputs_pruned.shape[1])\n",
    "    \n",
    "    model = GPy.models.GPRegression(X_train,y_train,kernel) \n",
    "    model.optimize()\n",
    "    mean, variance = model.predict(X_test)\n",
    "    \n",
    "    pred.append(mean)\n",
    "    std.append(100*np.divide(variance, y_test))\n",
    "    truth.append(y_test)\n",
    "#     print(i)\n",
    "\n",
    "\n",
    "truth = np.array(truth)\n",
    "pred = np.array(pred)\n",
    "pred = np.reshape(pred, (pred.shape[0]*pred.shape[1],1))\n",
    "pred = np.maximum(0,pred)\n",
    "truth = np.reshape(truth, (truth.shape[0]*truth.shape[1],1))\n",
    "print(\"median % relative error \", np.median(100*abs((pred-truth)/truth)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9bbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b805ed2c",
   "metadata": {},
   "source": [
    "# train all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f31ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_model = Sequential()  # instantiate the model \n",
    "peak_model = model_peak(inputs)  #give it the parameters given in the function model_dam\n",
    "\n",
    "lr_schedule = tfk.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.99)    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "opt = tfk.optimizers.RMSprop(learning_rate=lr_schedule,clipvalue=0.8)\n",
    "peak_model.compile(loss = customLoss, optimizer= opt, metrics=[customLoss]) #compile the model \n",
    "\n",
    "peak_model.fit(inputs_pruned, outputs_peak_pruned,epochs=1000, verbose = 0)\n",
    "\n",
    "sigma_f, l = 0.1,0.2\n",
    "kernel = GPy.kern.RBF(1, sigma_f, l) + GPy.kern.Matern32(input_dim=inputs_pruned.shape[1])\n",
    "model_shallow = GPy.models.GPRegression(inputs_pruned, outputs_pruned,kernel) \n",
    "model_shallow.optimize()\n",
    "\n",
    "\n",
    "double_peak_indicator = np.zeros((inputs.shape[0],1))\n",
    "for i in double_peak_samples:\n",
    "    double_peak_indicator[i] = 1\n",
    "\n",
    "double_peak_RF = RandomForestClassifier(max_depth=50, random_state= seed_value)\n",
    "double_peak_RF.fit(inputs, np.ravel(double_peak_indicator))\n",
    "\n",
    "\n",
    "\n",
    "print(\"trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ef0c8",
   "metadata": {},
   "source": [
    "# Identify best sample so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(nanoplates = 2):\n",
    "    nanoplates = nanoplates - 2\n",
    "    nanoplates = np.minimum(np.maximum(0,nanoplates),9)\n",
    "    lookup_table = [[427,438,435],[455,467,460],[472,479,477],[484,489,487],[491,498,496],[499,503,501],[504,507,505],[510,525,515]]\n",
    "    hard_lower, hard_upper, soft_middle = lookup_table[nanoplates]\n",
    "    return hard_lower, hard_upper, soft_middle\n",
    "\n",
    "\n",
    "def optimum_for_nanoplatelet(x, model_objective,outputs_peak_pruned , nanoplatelets = 2):\n",
    "    best_so_far = 0\n",
    "    lookup_table = [[427,438,435],[455,467,460],[472,479,477],[484,489,487],[491,498,496],[499,503,501],[504,507,505],[510,525,515]]\n",
    "\n",
    "    for i in range( outputs_peak_pruned.shape[0]):\n",
    "        if outputs_peak_pruned[i] >=lookup(nanoplatelets)[0] and outputs_peak_pruned[i] <=lookup(nanoplatelets)[1] :\n",
    "            pred_temp,_ = model_objective.predict(x[i:i+1])\n",
    "            if pred_temp > best_so_far:\n",
    "                best_so_far = pred_temp\n",
    "    return best_so_far[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754525bc",
   "metadata": {},
   "source": [
    "# BO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07022555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expected_improvement(X, optimal_so_far, gpr, tradeoff=0.01):\n",
    "    mu, sigma = gpr.predict(X)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu_sample_opt = optimal_so_far\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - tradeoff\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    return ei\n",
    "\n",
    "\n",
    "def get_score(Xsamples,optimal_so_far, model,peak_prediction, middle_constraints, experimental_constraints,method = \"pe\", nanoplate = 2,tradeoff = 0.5 ):\n",
    "    lower,upper,middle = lookup(nanoplate)\n",
    "    mu, std = model.predict(Xsamples)\n",
    "    mu = mu[:, 0]\n",
    "    std = std[:,0]\n",
    "    predicted_peak  = peak_prediction.predict(Xsamples)\n",
    "    std = np.reshape(std, (std.shape[0],1))\n",
    "    distance_from_inputs = np.ones((mu.shape[0],1))\n",
    "    mu = np.reshape(mu, (mu.shape[0],1))\n",
    "    if middle_constraints == 1:\n",
    "        constraint_middle = np.multiply(np.divide(abs(middle-predicted_peak),(upper-lower)),mu)\n",
    "\n",
    "    else:\n",
    "        constraint_middle = 0\n",
    "\n",
    "    predicted_feasibility = double_peak_RF.predict(Xsamples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    constrain_formability = np.exp(predicted_feasibility*sys.maxsize) - 1 \n",
    "    constrain_formability = np.reshape(constrain_formability, (constrain_formability.shape[0],1))\n",
    "    \n",
    "\n",
    "    constraints_terms = np.exp(sys.maxsize*( lower - predicted_peak)) +  np.exp(sys.maxsize*( predicted_peak - upper)) + constraint_middle + constrain_formability\n",
    "    if method == \"pi\":\n",
    "\n",
    "        scores = norm.cdf((mu - optimal_so_far) / (std+1E-9)) -constraints_terms  \n",
    "\n",
    "    elif method == \"pe\":\n",
    "        scores = expected_improvement(Xsamples,optimal_so_far, model, tradeoff)  - constraints_terms\n",
    "        \n",
    "    elif method == \"ucb\":\n",
    "        scores = mu + tradeoff*std - constraints_terms\n",
    "    return scores\n",
    "\n",
    "def get_suggestions(optimal_so_far, model, peak_prediction, lower_bound,middle_constraints = 1,experimental_constraints = 0,method = \"pe\", nanoplate = 2,tradeoff = 0.5):\n",
    "    Xsamples = []\n",
    "    sample_size = 100000\n",
    "    if experimental_constraints == 1:\n",
    "        acetone_constrained = (np.random.uniform(low=lower_bound, high=0.9-min_cons, size=(sample_size,1)))\n",
    "        b_temp = (np.random.uniform(low= lower_bound, high= 1-acetone_constrained-lower_bound, size=(sample_size,1)))\n",
    "        c_temp = 1 - acetone_constrained - b_temp\n",
    "        test = acetone_constrained + b_temp + c_temp\n",
    "\n",
    "        PbBr2_constrained = np.maximum(b_temp, c_temp)\n",
    "        oleate_constrained = np.minimum(b_temp, c_temp)\n",
    "        Xsamples = np.array(np.hstack((PbBr2_constrained,oleate_constrained,acetone_constrained)))\n",
    "    else:\n",
    "    \n",
    "        Xsamples = np.random.uniform(size=(sample_size,3))\n",
    "        summation = np.sum(Xsamples, axis = 1)\n",
    "        summation  = np.reshape(summation, (summation.shape[0],1))\n",
    "        Xsamples = Xsamples/ summation\n",
    "    scores = get_score(Xsamples,optimal_so_far, model,peak_prediction, middle_constraints,experimental_constraints, method,nanoplate, tradeoff)\n",
    "\n",
    "    ix = np.argmax(scores)\n",
    "    iy = np.argmin(scores)\n",
    "\n",
    "\n",
    "    if math.isinf((scores[ix])):\n",
    "        return np.array([0,0,0])\n",
    "    else:\n",
    "        return Xsamples[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b428d6f",
   "metadata": {},
   "source": [
    "# RUN BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f504",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acquisition_values = []\n",
    "experimental_constraints = 0\n",
    "tradeoff = 0.01  # higher tradeoff = more exploration \n",
    "for middle_const in [0]:\n",
    "    for method in [\"pe\"]:\n",
    "        for lower_bound in [0]:\n",
    "            print(\"middle constraitns = \", middle_const, \" method = \", method)\n",
    "            print()\n",
    "            for nano in [2,3,4,5,6,7,8,9]:\n",
    "                optimal_so_far = optimum_for_nanoplatelet(inputs_pruned,model_shallow, outputs_peak_pruned,nano)\n",
    "\n",
    "                if nano == 9:\n",
    "                    print(\"      nanocubes\")\n",
    "                else:\n",
    "                    print('      number of nanoplates : ',nano)\n",
    "                best_mean_shallow = 0\n",
    "                best_std_shallow = 0\n",
    "                for i in range(10): \n",
    "                    best = get_suggestions(optimal_so_far, model_shallow,peak_model,lower_bound,middle_const,experimental_constraints,method,nano, tradeoff)\n",
    "                    best = np.reshape(best, (1,best.shape[0]))\n",
    "                    results = model_shallow.predict(best)\n",
    "                    acquisition_values.append(best)\n",
    "                    mean_shallow = results[0][0][0]\n",
    "                    std_shallow = results[1][0][0]\n",
    "                    if mean_shallow > best_mean_shallow:\n",
    "                        sample_mean_shallow = best\n",
    "                        best_mean_shallow = mean_shallow\n",
    "                    if std_shallow > best_std_shallow:\n",
    "                        best_std_shallow = std_shallow\n",
    "                        sample_std_shallow = best\n",
    "                print(\"           spectrum range \",lookup(nano))\n",
    "                print(\"           composition \", sample_mean_shallow)\n",
    "                print(\"           objective function \", model_shallow.predict(sample_mean_shallow)[0][0][0], 'uncertainty ', model_shallow.predict(sample_mean_shallow)[1][0][0])\n",
    "                print(\"           predicted peak \",peak_model.predict(sample_mean_shallow)[0][0])\n",
    "                print(\"           optimal so far \", optimal_so_far)\n",
    "                print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml",
   "language": "python",
   "name": "general_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
